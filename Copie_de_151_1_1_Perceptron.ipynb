{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hocine-drioueche/OpenclassroomsProject/blob/main/Copie_de_151_1_1_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtYbBwLwU2Dy"
      },
      "source": [
        "<img src=\"https://datascientest.fr/train/assets/logo_datascientest.png\" style=\"height:150px\">\n",
        "\n",
        "\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "<center><h1>Introduction au Deep Learning avec Keras</h1></center>\n",
        "<center><h2>L'algorithme du Perceptron</h2></center>\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "\n",
        "\n",
        "> Dans cet exercice, nous allons introduire le principe de fonctionnement de l'algorithme du perceptron. Pour se faire, nous allons entraîner un modèle basé sur le principe du perceptron simple à partir de la base de données *Moon* de la bibliothèque *scikit-learn*.\n",
        "\n",
        "> Comme évoqué précédemment, le **choix** de la fonction d'activation dépend de **l'espace de sortie souhaité** du modèle Perceptron:\n",
        ">\n",
        "> * La fonction $\\mathbf{sigmoid}$ prend des valeurs de $(-\\infty, \\infty) $ à $ [0,1]$, ce qui en fait un choix idéal pour afficher une valeur de probabilité pour la classification. Dans ce cas, le modèle Perceptron équivaut à la **Régression logistique**.\n",
        ">\n",
        ">Dans l'exemple ci-dessous, nous allons implémenter ce modèle.\n",
        ">\n",
        "\n",
        "\n",
        "* **(a)** Exécuter la cellule suivante pour importer les packages nécéssaires :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "LilXIKXrU2Dz"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "sn971gcrU2D0"
      },
      "source": [
        "* **(b)** Exécuter la cellule suivante pour stocker dans X (**features**) et y (**target**) un échantillon du dataset *Moon* à l'aide du module make_moons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "nKtd7bT8U2D0"
      },
      "source": [
        "X,y = make_moons(n_samples = 300, noise = 0.05) #300 observations choisies pour ne pas trop alourdir l'entraînement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUN7KrWyU2D0"
      },
      "source": [
        "* **(c)** Afficher les observations en colorant chacune d'entre elles selon la classe à laquelle elle appartient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brRHIG-AU2D0"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ay67SZPZU2D1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "c69076df-d833-41eb-8333-beb954a08538"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "X_0_x = X[y == 0][:, 0]\n",
        "X_0_y = X[y == 0][:, 1]\n",
        "\n",
        "X_1_x = X[y == 1][:, 0]\n",
        "X_1_y = X[y == 1][:, 1]\n",
        "\n",
        "ax.scatter(X_0_x, X_0_y, color = \"#68E2BF\", label = \"Class 0\")\n",
        "ax.scatter(X_1_x, X_1_y, color = \"#163090\", label = \"Class 1\")\n",
        "\n",
        "ax.legend()\n",
        "fig.set_size_inches(8, 4.5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x450 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaTOFdZSU2D1"
      },
      "source": [
        "* **(d)** Séparer les données en un jeu d'entraiement **80%** et de test **20%**. Utiliser l'argument **`random_state=42`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWrjIJ5U2D1"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Q_IEiDIKU2D1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test, y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "Tj9qRuDMU2D1"
      },
      "source": [
        ">Nous allons maintenant définir le 'neurone' ou perceptron par lequel passer nos données.\n",
        ">\n",
        "* Exécuter la cellule suivante afin d'importer les packages nécéssaires à l'initialisation du perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "QuxpaIpkU2D2"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense #Pour instancier une couche Dense et une d'Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYLwL8-AU2D2"
      },
      "source": [
        ">Dans un premier temps, nous allons définir l'**`Input`** qui prendra en compte la dimension des variables d'entrée, c'est à dire la dimension de nos variables explicatives. Les objets **Input** et **Output**, de classe *Keras Tensor*, permettent de construire des modèles à partir seulement des entrées.\n",
        ">\n",
        "* **(e)** Instancier cet **`Input`** dans inputs.\n",
        "* **(f)** Créer une couche **`Dense`** nommée **`dense1`** prenant en argument **`units=1`**, qui correspond au nombre de perceptrons de notre couche, une fonction d'activation **`sigmoid`** ainsi que l'argument **`kernel_regularizer=regularizers.l2(0.)`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS8dghYnU2D2"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5-h7cxKuU2D2"
      },
      "source": [
        "inputs = Input(shape = 2, name = \"Input\")\n",
        "\n",
        "dense1 = Dense(units = 1, activation = \"sigmoid\",kernel_regularizer=regularizers.l2(0.), name = \"Dense_1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zQiC-vWU2D2"
      },
      "source": [
        ">Nous allons à présent définir l'**`output`** de notre modèle. Comme ici, il n'est constitué que d'un seul perceptron, l'output correspond à notre input ) laquelle on a appliqué la couche **`dense1`**.\n",
        ">\n",
        "* **(g)** Dans **`outputs`**, appliquer la couche **`dense1`** sur L'**`Input`** précédent.\n",
        "* **(h)** Instancier un **Model** prenant en argument l'**`Input`** et l'**`output`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nifVFWlRU2D3"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dbys-zIKU2D3"
      },
      "source": [
        "outputs=dense1(inputs)\n",
        "\n",
        "model = Model(inputs = inputs, outputs = outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZpMdirLU2D3"
      },
      "source": [
        "* **(i)** Compiler le modèle précedemment créé grâce à la méthode **`.compile`**. Utiliser comme fonction de perte : **\"binary_crossentropy\"**, comme optimiseur : **\"adam\"** et pour métrique : **[\"accuracy\"]**.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "<i class=\"fa fa-info-circle\"></i> &emsp;\n",
        "    <b>Rappel</b> : Une fonction de perte (le terme perte a été utilisé pour la première fois par Wald, 1939) permet de mesurer  <b>un écart</b> entre les valeurs observées des données et les valeurs calculées à l'aide de la fonction d'ajustement. C'est la fonction que l'on cherche à <b>minimiser pendant l'entraînement d'un modèle</b>.     \n",
        "    <br> Dans le cadre de problèmes de classification binaire, on privilégie en général la fonction de perte binary crossentropy. On aura plus tendance à regarder l'erreur absolue ou quadratique moyenne pour des problèmes de régression.\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3T0sQfCU2D4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDXm8okHU2D4"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3C4IG3ldU2D4"
      },
      "source": [
        "model.compile(loss = \"binary_crossentropy\",\n",
        "              optimizer = \"adam\",\n",
        "              metrics = [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQBskc6uU2D4"
      },
      "source": [
        "* **(j)** Entrainer votre modèle sur le jeu de données d'entrainement avec comme argument : epochs=500, batch_size=32, validation_split=0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV2d5EV6U2D4"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Lt15vsPnU2D5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8aef1cc-764d-4441-e346-f876930aa30a"
      },
      "source": [
        "model.fit(X_train,y_train,epochs=500,batch_size=2, validation_split=0.2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "96/96 [==============================] - 1s 4ms/step - loss: 0.7995 - accuracy: 0.5990 - val_loss: 0.7693 - val_accuracy: 0.6458\n",
            "Epoch 2/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.7730 - accuracy: 0.5990 - val_loss: 0.7466 - val_accuracy: 0.6458\n",
            "Epoch 3/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.7476 - accuracy: 0.6042 - val_loss: 0.7258 - val_accuracy: 0.6458\n",
            "Epoch 4/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7238 - accuracy: 0.6146 - val_loss: 0.7065 - val_accuracy: 0.5833\n",
            "Epoch 5/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7018 - accuracy: 0.6146 - val_loss: 0.6874 - val_accuracy: 0.5208\n",
            "Epoch 6/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.6250 - val_loss: 0.6709 - val_accuracy: 0.5417\n",
            "Epoch 7/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.6614 - accuracy: 0.5938 - val_loss: 0.6545 - val_accuracy: 0.5208\n",
            "Epoch 8/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6432 - accuracy: 0.5938 - val_loss: 0.6395 - val_accuracy: 0.5000\n",
            "Epoch 9/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6265 - accuracy: 0.5885 - val_loss: 0.6252 - val_accuracy: 0.5417\n",
            "Epoch 10/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6107 - accuracy: 0.5938 - val_loss: 0.6123 - val_accuracy: 0.5625\n",
            "Epoch 11/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.5959 - accuracy: 0.5990 - val_loss: 0.6005 - val_accuracy: 0.5833\n",
            "Epoch 12/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.5824 - accuracy: 0.6198 - val_loss: 0.5888 - val_accuracy: 0.5833\n",
            "Epoch 13/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.5694 - accuracy: 0.6302 - val_loss: 0.5780 - val_accuracy: 0.5833\n",
            "Epoch 14/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.6354 - val_loss: 0.5686 - val_accuracy: 0.5833\n",
            "Epoch 15/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.5459 - accuracy: 0.6406 - val_loss: 0.5590 - val_accuracy: 0.6042\n",
            "Epoch 16/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.6510 - val_loss: 0.5501 - val_accuracy: 0.6250\n",
            "Epoch 17/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.6510 - val_loss: 0.5417 - val_accuracy: 0.6250\n",
            "Epoch 18/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.6667 - val_loss: 0.5338 - val_accuracy: 0.6250\n",
            "Epoch 19/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.6927 - val_loss: 0.5261 - val_accuracy: 0.6250\n",
            "Epoch 20/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4980 - accuracy: 0.7031 - val_loss: 0.5191 - val_accuracy: 0.6250\n",
            "Epoch 21/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.7031 - val_loss: 0.5124 - val_accuracy: 0.6458\n",
            "Epoch 22/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.7240 - val_loss: 0.5059 - val_accuracy: 0.6667\n",
            "Epoch 23/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4750 - accuracy: 0.7240 - val_loss: 0.4996 - val_accuracy: 0.6667\n",
            "Epoch 24/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4680 - accuracy: 0.7292 - val_loss: 0.4937 - val_accuracy: 0.6667\n",
            "Epoch 25/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.7344 - val_loss: 0.4881 - val_accuracy: 0.6875\n",
            "Epoch 26/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.7396 - val_loss: 0.4827 - val_accuracy: 0.6875\n",
            "Epoch 27/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.7396 - val_loss: 0.4773 - val_accuracy: 0.6875\n",
            "Epoch 28/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.7448 - val_loss: 0.4723 - val_accuracy: 0.6875\n",
            "Epoch 29/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.7604 - val_loss: 0.4675 - val_accuracy: 0.6875\n",
            "Epoch 30/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4321 - accuracy: 0.7656 - val_loss: 0.4629 - val_accuracy: 0.6875\n",
            "Epoch 31/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4270 - accuracy: 0.7708 - val_loss: 0.4583 - val_accuracy: 0.7083\n",
            "Epoch 32/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4218 - accuracy: 0.7708 - val_loss: 0.4540 - val_accuracy: 0.7083\n",
            "Epoch 33/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4171 - accuracy: 0.7760 - val_loss: 0.4499 - val_accuracy: 0.7083\n",
            "Epoch 34/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.7760 - val_loss: 0.4458 - val_accuracy: 0.7083\n",
            "Epoch 35/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.7760 - val_loss: 0.4419 - val_accuracy: 0.7083\n",
            "Epoch 36/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.7865 - val_loss: 0.4380 - val_accuracy: 0.7083\n",
            "Epoch 37/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.7865 - val_loss: 0.4344 - val_accuracy: 0.7083\n",
            "Epoch 38/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.7865 - val_loss: 0.4307 - val_accuracy: 0.7292\n",
            "Epoch 39/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3917 - accuracy: 0.7865 - val_loss: 0.4273 - val_accuracy: 0.7292\n",
            "Epoch 40/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.7865 - val_loss: 0.4239 - val_accuracy: 0.7292\n",
            "Epoch 41/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.7917 - val_loss: 0.4207 - val_accuracy: 0.7292\n",
            "Epoch 42/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.7917 - val_loss: 0.4175 - val_accuracy: 0.7292\n",
            "Epoch 43/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.8021 - val_loss: 0.4143 - val_accuracy: 0.7292\n",
            "Epoch 44/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3744 - accuracy: 0.8073 - val_loss: 0.4114 - val_accuracy: 0.7292\n",
            "Epoch 45/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8073 - val_loss: 0.4084 - val_accuracy: 0.7292\n",
            "Epoch 46/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.8073 - val_loss: 0.4055 - val_accuracy: 0.7292\n",
            "Epoch 47/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3651 - accuracy: 0.8073 - val_loss: 0.4028 - val_accuracy: 0.7292\n",
            "Epoch 48/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3623 - accuracy: 0.8125 - val_loss: 0.4001 - val_accuracy: 0.7500\n",
            "Epoch 49/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8125 - val_loss: 0.3973 - val_accuracy: 0.7500\n",
            "Epoch 50/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3567 - accuracy: 0.8125 - val_loss: 0.3948 - val_accuracy: 0.7500\n",
            "Epoch 51/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3542 - accuracy: 0.8125 - val_loss: 0.3923 - val_accuracy: 0.7500\n",
            "Epoch 52/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.8125 - val_loss: 0.3899 - val_accuracy: 0.7500\n",
            "Epoch 53/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.8125 - val_loss: 0.3874 - val_accuracy: 0.7708\n",
            "Epoch 54/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3469 - accuracy: 0.8177 - val_loss: 0.3852 - val_accuracy: 0.7708\n",
            "Epoch 55/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8177 - val_loss: 0.3829 - val_accuracy: 0.7708\n",
            "Epoch 56/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3422 - accuracy: 0.8177 - val_loss: 0.3808 - val_accuracy: 0.7708\n",
            "Epoch 57/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.8177 - val_loss: 0.3784 - val_accuracy: 0.7917\n",
            "Epoch 58/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8177 - val_loss: 0.3765 - val_accuracy: 0.7917\n",
            "Epoch 59/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8177 - val_loss: 0.3744 - val_accuracy: 0.7917\n",
            "Epoch 60/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3339 - accuracy: 0.8177 - val_loss: 0.3724 - val_accuracy: 0.7917\n",
            "Epoch 61/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8177 - val_loss: 0.3705 - val_accuracy: 0.7917\n",
            "Epoch 62/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3301 - accuracy: 0.8229 - val_loss: 0.3685 - val_accuracy: 0.7917\n",
            "Epoch 63/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.8229 - val_loss: 0.3667 - val_accuracy: 0.7917\n",
            "Epoch 64/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8229 - val_loss: 0.3648 - val_accuracy: 0.7917\n",
            "Epoch 65/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8229 - val_loss: 0.3631 - val_accuracy: 0.7917\n",
            "Epoch 66/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8229 - val_loss: 0.3613 - val_accuracy: 0.7917\n",
            "Epoch 67/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3215 - accuracy: 0.8333 - val_loss: 0.3597 - val_accuracy: 0.7917\n",
            "Epoch 68/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 0.8333 - val_loss: 0.3581 - val_accuracy: 0.7917\n",
            "Epoch 69/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8333 - val_loss: 0.3563 - val_accuracy: 0.7917\n",
            "Epoch 70/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3169 - accuracy: 0.8333 - val_loss: 0.3549 - val_accuracy: 0.7917\n",
            "Epoch 71/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8333 - val_loss: 0.3533 - val_accuracy: 0.7917\n",
            "Epoch 72/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.8385 - val_loss: 0.3518 - val_accuracy: 0.7917\n",
            "Epoch 73/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3128 - accuracy: 0.8438 - val_loss: 0.3503 - val_accuracy: 0.7917\n",
            "Epoch 74/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8438 - val_loss: 0.3489 - val_accuracy: 0.8125\n",
            "Epoch 75/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8438 - val_loss: 0.3475 - val_accuracy: 0.8125\n",
            "Epoch 76/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8490 - val_loss: 0.3462 - val_accuracy: 0.8125\n",
            "Epoch 77/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.8490 - val_loss: 0.3448 - val_accuracy: 0.8125\n",
            "Epoch 78/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8542 - val_loss: 0.3436 - val_accuracy: 0.8125\n",
            "Epoch 79/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3055 - accuracy: 0.8542 - val_loss: 0.3422 - val_accuracy: 0.8125\n",
            "Epoch 80/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3043 - accuracy: 0.8594 - val_loss: 0.3410 - val_accuracy: 0.8125\n",
            "Epoch 81/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3032 - accuracy: 0.8594 - val_loss: 0.3398 - val_accuracy: 0.8333\n",
            "Epoch 82/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3022 - accuracy: 0.8594 - val_loss: 0.3386 - val_accuracy: 0.8333\n",
            "Epoch 83/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3012 - accuracy: 0.8594 - val_loss: 0.3375 - val_accuracy: 0.8333\n",
            "Epoch 84/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8594 - val_loss: 0.3364 - val_accuracy: 0.8333\n",
            "Epoch 85/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2993 - accuracy: 0.8594 - val_loss: 0.3354 - val_accuracy: 0.8333\n",
            "Epoch 86/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8594 - val_loss: 0.3342 - val_accuracy: 0.8542\n",
            "Epoch 87/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2974 - accuracy: 0.8594 - val_loss: 0.3333 - val_accuracy: 0.8542\n",
            "Epoch 88/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2966 - accuracy: 0.8594 - val_loss: 0.3322 - val_accuracy: 0.8542\n",
            "Epoch 89/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2957 - accuracy: 0.8646 - val_loss: 0.3312 - val_accuracy: 0.8542\n",
            "Epoch 90/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.8646 - val_loss: 0.3302 - val_accuracy: 0.8542\n",
            "Epoch 91/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2941 - accuracy: 0.8698 - val_loss: 0.3292 - val_accuracy: 0.8542\n",
            "Epoch 92/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2933 - accuracy: 0.8646 - val_loss: 0.3282 - val_accuracy: 0.8542\n",
            "Epoch 93/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8646 - val_loss: 0.3274 - val_accuracy: 0.8542\n",
            "Epoch 94/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2917 - accuracy: 0.8698 - val_loss: 0.3264 - val_accuracy: 0.8542\n",
            "Epoch 95/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2911 - accuracy: 0.8698 - val_loss: 0.3256 - val_accuracy: 0.8542\n",
            "Epoch 96/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2903 - accuracy: 0.8698 - val_loss: 0.3247 - val_accuracy: 0.8542\n",
            "Epoch 97/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2896 - accuracy: 0.8698 - val_loss: 0.3240 - val_accuracy: 0.8542\n",
            "Epoch 98/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2890 - accuracy: 0.8698 - val_loss: 0.3231 - val_accuracy: 0.8542\n",
            "Epoch 99/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2883 - accuracy: 0.8698 - val_loss: 0.3223 - val_accuracy: 0.8542\n",
            "Epoch 100/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.8698 - val_loss: 0.3216 - val_accuracy: 0.8542\n",
            "Epoch 101/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.8698 - val_loss: 0.3207 - val_accuracy: 0.8542\n",
            "Epoch 102/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2865 - accuracy: 0.8698 - val_loss: 0.3201 - val_accuracy: 0.8542\n",
            "Epoch 103/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2859 - accuracy: 0.8698 - val_loss: 0.3193 - val_accuracy: 0.8542\n",
            "Epoch 104/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2853 - accuracy: 0.8698 - val_loss: 0.3186 - val_accuracy: 0.8542\n",
            "Epoch 105/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.8698 - val_loss: 0.3178 - val_accuracy: 0.8542\n",
            "Epoch 106/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2842 - accuracy: 0.8698 - val_loss: 0.3172 - val_accuracy: 0.8542\n",
            "Epoch 107/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.8698 - val_loss: 0.3166 - val_accuracy: 0.8542\n",
            "Epoch 108/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2833 - accuracy: 0.8698 - val_loss: 0.3159 - val_accuracy: 0.8542\n",
            "Epoch 109/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2827 - accuracy: 0.8698 - val_loss: 0.3152 - val_accuracy: 0.8542\n",
            "Epoch 110/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2823 - accuracy: 0.8698 - val_loss: 0.3146 - val_accuracy: 0.8542\n",
            "Epoch 111/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2817 - accuracy: 0.8698 - val_loss: 0.3139 - val_accuracy: 0.8542\n",
            "Epoch 112/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2813 - accuracy: 0.8698 - val_loss: 0.3134 - val_accuracy: 0.8542\n",
            "Epoch 113/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2808 - accuracy: 0.8698 - val_loss: 0.3127 - val_accuracy: 0.8542\n",
            "Epoch 114/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2804 - accuracy: 0.8698 - val_loss: 0.3123 - val_accuracy: 0.8542\n",
            "Epoch 115/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.8698 - val_loss: 0.3117 - val_accuracy: 0.8542\n",
            "Epoch 116/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.8698 - val_loss: 0.3112 - val_accuracy: 0.8542\n",
            "Epoch 117/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2792 - accuracy: 0.8698 - val_loss: 0.3106 - val_accuracy: 0.8542\n",
            "Epoch 118/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.8698 - val_loss: 0.3100 - val_accuracy: 0.8542\n",
            "Epoch 119/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2784 - accuracy: 0.8698 - val_loss: 0.3096 - val_accuracy: 0.8542\n",
            "Epoch 120/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2780 - accuracy: 0.8698 - val_loss: 0.3090 - val_accuracy: 0.8542\n",
            "Epoch 121/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.8698 - val_loss: 0.3086 - val_accuracy: 0.8542\n",
            "Epoch 122/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.8698 - val_loss: 0.3081 - val_accuracy: 0.8542\n",
            "Epoch 123/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.8698 - val_loss: 0.3076 - val_accuracy: 0.8750\n",
            "Epoch 124/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2765 - accuracy: 0.8698 - val_loss: 0.3071 - val_accuracy: 0.8750\n",
            "Epoch 125/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2763 - accuracy: 0.8698 - val_loss: 0.3067 - val_accuracy: 0.8750\n",
            "Epoch 126/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2758 - accuracy: 0.8698 - val_loss: 0.3062 - val_accuracy: 0.8750\n",
            "Epoch 127/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8698 - val_loss: 0.3058 - val_accuracy: 0.8750\n",
            "Epoch 128/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2753 - accuracy: 0.8698 - val_loss: 0.3053 - val_accuracy: 0.8750\n",
            "Epoch 129/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8698 - val_loss: 0.3049 - val_accuracy: 0.8750\n",
            "Epoch 130/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2747 - accuracy: 0.8698 - val_loss: 0.3045 - val_accuracy: 0.8750\n",
            "Epoch 131/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.8698 - val_loss: 0.3041 - val_accuracy: 0.8750\n",
            "Epoch 132/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2742 - accuracy: 0.8698 - val_loss: 0.3037 - val_accuracy: 0.8750\n",
            "Epoch 133/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2739 - accuracy: 0.8698 - val_loss: 0.3033 - val_accuracy: 0.8750\n",
            "Epoch 134/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2735 - accuracy: 0.8698 - val_loss: 0.3029 - val_accuracy: 0.8750\n",
            "Epoch 135/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2733 - accuracy: 0.8698 - val_loss: 0.3026 - val_accuracy: 0.8750\n",
            "Epoch 136/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2730 - accuracy: 0.8698 - val_loss: 0.3022 - val_accuracy: 0.8750\n",
            "Epoch 137/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8698 - val_loss: 0.3018 - val_accuracy: 0.8750\n",
            "Epoch 138/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.8698 - val_loss: 0.3015 - val_accuracy: 0.8750\n",
            "Epoch 139/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2722 - accuracy: 0.8698 - val_loss: 0.3011 - val_accuracy: 0.8750\n",
            "Epoch 140/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.8698 - val_loss: 0.3008 - val_accuracy: 0.8750\n",
            "Epoch 141/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2719 - accuracy: 0.8698 - val_loss: 0.3004 - val_accuracy: 0.8750\n",
            "Epoch 142/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2715 - accuracy: 0.8698 - val_loss: 0.3001 - val_accuracy: 0.8750\n",
            "Epoch 143/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.8698 - val_loss: 0.2998 - val_accuracy: 0.8750\n",
            "Epoch 144/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2713 - accuracy: 0.8698 - val_loss: 0.2995 - val_accuracy: 0.8750\n",
            "Epoch 145/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2709 - accuracy: 0.8698 - val_loss: 0.2992 - val_accuracy: 0.8750\n",
            "Epoch 146/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.8698 - val_loss: 0.2988 - val_accuracy: 0.8750\n",
            "Epoch 147/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.8698 - val_loss: 0.2986 - val_accuracy: 0.8750\n",
            "Epoch 148/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.8698 - val_loss: 0.2983 - val_accuracy: 0.8750\n",
            "Epoch 149/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.8698 - val_loss: 0.2980 - val_accuracy: 0.8750\n",
            "Epoch 150/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.8698 - val_loss: 0.2977 - val_accuracy: 0.8750\n",
            "Epoch 151/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.8698 - val_loss: 0.2975 - val_accuracy: 0.8750\n",
            "Epoch 152/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.8698 - val_loss: 0.2972 - val_accuracy: 0.8750\n",
            "Epoch 153/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.8698 - val_loss: 0.2969 - val_accuracy: 0.8750\n",
            "Epoch 154/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8698 - val_loss: 0.2966 - val_accuracy: 0.8750\n",
            "Epoch 155/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8698 - val_loss: 0.2964 - val_accuracy: 0.8750\n",
            "Epoch 156/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2690 - accuracy: 0.8698 - val_loss: 0.2961 - val_accuracy: 0.8750\n",
            "Epoch 157/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.8698 - val_loss: 0.2959 - val_accuracy: 0.8750\n",
            "Epoch 158/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2685 - accuracy: 0.8698 - val_loss: 0.2956 - val_accuracy: 0.8750\n",
            "Epoch 159/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2684 - accuracy: 0.8698 - val_loss: 0.2955 - val_accuracy: 0.8750\n",
            "Epoch 160/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2683 - accuracy: 0.8698 - val_loss: 0.2952 - val_accuracy: 0.8750\n",
            "Epoch 161/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.8698 - val_loss: 0.2950 - val_accuracy: 0.8750\n",
            "Epoch 162/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.8698 - val_loss: 0.2948 - val_accuracy: 0.8750\n",
            "Epoch 163/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.8698 - val_loss: 0.2945 - val_accuracy: 0.8750\n",
            "Epoch 164/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8698 - val_loss: 0.2943 - val_accuracy: 0.8750\n",
            "Epoch 165/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.8698 - val_loss: 0.2941 - val_accuracy: 0.8750\n",
            "Epoch 166/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.8698 - val_loss: 0.2939 - val_accuracy: 0.8750\n",
            "Epoch 167/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2673 - accuracy: 0.8698 - val_loss: 0.2937 - val_accuracy: 0.8750\n",
            "Epoch 168/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2672 - accuracy: 0.8698 - val_loss: 0.2935 - val_accuracy: 0.8750\n",
            "Epoch 169/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.8698 - val_loss: 0.2933 - val_accuracy: 0.8750\n",
            "Epoch 170/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2670 - accuracy: 0.8698 - val_loss: 0.2931 - val_accuracy: 0.8750\n",
            "Epoch 171/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2669 - accuracy: 0.8698 - val_loss: 0.2929 - val_accuracy: 0.8750\n",
            "Epoch 172/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.8698 - val_loss: 0.2927 - val_accuracy: 0.8750\n",
            "Epoch 173/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.8698 - val_loss: 0.2925 - val_accuracy: 0.8750\n",
            "Epoch 174/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8698 - val_loss: 0.2924 - val_accuracy: 0.8750\n",
            "Epoch 175/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.8750 - val_loss: 0.2922 - val_accuracy: 0.8750\n",
            "Epoch 176/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.8750 - val_loss: 0.2920 - val_accuracy: 0.8750\n",
            "Epoch 177/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2662 - accuracy: 0.8750 - val_loss: 0.2919 - val_accuracy: 0.8750\n",
            "Epoch 178/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2661 - accuracy: 0.8750 - val_loss: 0.2917 - val_accuracy: 0.8750\n",
            "Epoch 179/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2661 - accuracy: 0.8802 - val_loss: 0.2915 - val_accuracy: 0.8750\n",
            "Epoch 180/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.8802 - val_loss: 0.2913 - val_accuracy: 0.8750\n",
            "Epoch 181/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.8802 - val_loss: 0.2912 - val_accuracy: 0.8750\n",
            "Epoch 182/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.8802 - val_loss: 0.2910 - val_accuracy: 0.8750\n",
            "Epoch 183/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8802 - val_loss: 0.2909 - val_accuracy: 0.8750\n",
            "Epoch 184/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8802 - val_loss: 0.2907 - val_accuracy: 0.8750\n",
            "Epoch 185/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2656 - accuracy: 0.8802 - val_loss: 0.2906 - val_accuracy: 0.8750\n",
            "Epoch 186/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.8802 - val_loss: 0.2904 - val_accuracy: 0.8750\n",
            "Epoch 187/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.8802 - val_loss: 0.2903 - val_accuracy: 0.8750\n",
            "Epoch 188/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8802 - val_loss: 0.2902 - val_accuracy: 0.8750\n",
            "Epoch 189/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2652 - accuracy: 0.8802 - val_loss: 0.2900 - val_accuracy: 0.8750\n",
            "Epoch 190/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.8802 - val_loss: 0.2899 - val_accuracy: 0.8750\n",
            "Epoch 191/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.8802 - val_loss: 0.2897 - val_accuracy: 0.8750\n",
            "Epoch 192/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.8802 - val_loss: 0.2896 - val_accuracy: 0.8750\n",
            "Epoch 193/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.8802 - val_loss: 0.2894 - val_accuracy: 0.8750\n",
            "Epoch 194/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.8802 - val_loss: 0.2893 - val_accuracy: 0.8750\n",
            "Epoch 195/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.8802 - val_loss: 0.2892 - val_accuracy: 0.8750\n",
            "Epoch 196/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.8802 - val_loss: 0.2891 - val_accuracy: 0.8750\n",
            "Epoch 197/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.8802 - val_loss: 0.2889 - val_accuracy: 0.8750\n",
            "Epoch 198/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2645 - accuracy: 0.8802 - val_loss: 0.2888 - val_accuracy: 0.8750\n",
            "Epoch 199/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8802 - val_loss: 0.2887 - val_accuracy: 0.8750\n",
            "Epoch 200/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2643 - accuracy: 0.8802 - val_loss: 0.2886 - val_accuracy: 0.8750\n",
            "Epoch 201/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.8802 - val_loss: 0.2885 - val_accuracy: 0.8750\n",
            "Epoch 202/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.8802 - val_loss: 0.2884 - val_accuracy: 0.8750\n",
            "Epoch 203/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2641 - accuracy: 0.8802 - val_loss: 0.2882 - val_accuracy: 0.8750\n",
            "Epoch 204/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.8802 - val_loss: 0.2881 - val_accuracy: 0.8750\n",
            "Epoch 205/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2639 - accuracy: 0.8802 - val_loss: 0.2880 - val_accuracy: 0.8750\n",
            "Epoch 206/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2639 - accuracy: 0.8802 - val_loss: 0.2879 - val_accuracy: 0.8750\n",
            "Epoch 207/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.8802 - val_loss: 0.2878 - val_accuracy: 0.8750\n",
            "Epoch 208/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.8802 - val_loss: 0.2877 - val_accuracy: 0.8750\n",
            "Epoch 209/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.8802 - val_loss: 0.2876 - val_accuracy: 0.8750\n",
            "Epoch 210/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.8802 - val_loss: 0.2875 - val_accuracy: 0.8750\n",
            "Epoch 211/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2636 - accuracy: 0.8802 - val_loss: 0.2874 - val_accuracy: 0.8750\n",
            "Epoch 212/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2636 - accuracy: 0.8802 - val_loss: 0.2873 - val_accuracy: 0.8750\n",
            "Epoch 213/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2636 - accuracy: 0.8802 - val_loss: 0.2872 - val_accuracy: 0.8750\n",
            "Epoch 214/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2634 - accuracy: 0.8802 - val_loss: 0.2871 - val_accuracy: 0.8750\n",
            "Epoch 215/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2634 - accuracy: 0.8802 - val_loss: 0.2870 - val_accuracy: 0.8750\n",
            "Epoch 216/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2635 - accuracy: 0.8802 - val_loss: 0.2869 - val_accuracy: 0.8750\n",
            "Epoch 217/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2634 - accuracy: 0.8802 - val_loss: 0.2868 - val_accuracy: 0.8750\n",
            "Epoch 218/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2632 - accuracy: 0.8802 - val_loss: 0.2867 - val_accuracy: 0.8750\n",
            "Epoch 219/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2632 - accuracy: 0.8802 - val_loss: 0.2867 - val_accuracy: 0.8750\n",
            "Epoch 220/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2633 - accuracy: 0.8802 - val_loss: 0.2866 - val_accuracy: 0.8750\n",
            "Epoch 221/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2631 - accuracy: 0.8802 - val_loss: 0.2865 - val_accuracy: 0.8750\n",
            "Epoch 222/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2632 - accuracy: 0.8802 - val_loss: 0.2864 - val_accuracy: 0.8750\n",
            "Epoch 223/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2631 - accuracy: 0.8802 - val_loss: 0.2863 - val_accuracy: 0.8750\n",
            "Epoch 224/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.8802 - val_loss: 0.2862 - val_accuracy: 0.8750\n",
            "Epoch 225/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2631 - accuracy: 0.8802 - val_loss: 0.2862 - val_accuracy: 0.8750\n",
            "Epoch 226/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.8802 - val_loss: 0.2861 - val_accuracy: 0.8750\n",
            "Epoch 227/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.8802 - val_loss: 0.2860 - val_accuracy: 0.8750\n",
            "Epoch 228/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2628 - accuracy: 0.8802 - val_loss: 0.2859 - val_accuracy: 0.8750\n",
            "Epoch 229/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2628 - accuracy: 0.8802 - val_loss: 0.2859 - val_accuracy: 0.8750\n",
            "Epoch 230/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2628 - accuracy: 0.8802 - val_loss: 0.2858 - val_accuracy: 0.8750\n",
            "Epoch 231/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2628 - accuracy: 0.8802 - val_loss: 0.2857 - val_accuracy: 0.8750\n",
            "Epoch 232/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2628 - accuracy: 0.8802 - val_loss: 0.2856 - val_accuracy: 0.8750\n",
            "Epoch 233/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8802 - val_loss: 0.2855 - val_accuracy: 0.8750\n",
            "Epoch 234/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2628 - accuracy: 0.8802 - val_loss: 0.2855 - val_accuracy: 0.8750\n",
            "Epoch 235/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.8802 - val_loss: 0.2854 - val_accuracy: 0.8750\n",
            "Epoch 236/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.8802 - val_loss: 0.2853 - val_accuracy: 0.8750\n",
            "Epoch 237/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.8802 - val_loss: 0.2853 - val_accuracy: 0.8750\n",
            "Epoch 238/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2625 - accuracy: 0.8802 - val_loss: 0.2852 - val_accuracy: 0.8750\n",
            "Epoch 239/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2625 - accuracy: 0.8802 - val_loss: 0.2851 - val_accuracy: 0.8750\n",
            "Epoch 240/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2624 - accuracy: 0.8802 - val_loss: 0.2851 - val_accuracy: 0.8750\n",
            "Epoch 241/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2624 - accuracy: 0.8802 - val_loss: 0.2850 - val_accuracy: 0.8750\n",
            "Epoch 242/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.8802 - val_loss: 0.2850 - val_accuracy: 0.8750\n",
            "Epoch 243/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.8802 - val_loss: 0.2849 - val_accuracy: 0.8750\n",
            "Epoch 244/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.8802 - val_loss: 0.2848 - val_accuracy: 0.8750\n",
            "Epoch 245/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.8802 - val_loss: 0.2848 - val_accuracy: 0.8750\n",
            "Epoch 246/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.8802 - val_loss: 0.2847 - val_accuracy: 0.8750\n",
            "Epoch 247/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.8802 - val_loss: 0.2847 - val_accuracy: 0.8750\n",
            "Epoch 248/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.8802 - val_loss: 0.2846 - val_accuracy: 0.8750\n",
            "Epoch 249/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.8802 - val_loss: 0.2845 - val_accuracy: 0.8750\n",
            "Epoch 250/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.8802 - val_loss: 0.2845 - val_accuracy: 0.8750\n",
            "Epoch 251/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.8802 - val_loss: 0.2844 - val_accuracy: 0.8750\n",
            "Epoch 252/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.8802 - val_loss: 0.2844 - val_accuracy: 0.8750\n",
            "Epoch 253/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.8802 - val_loss: 0.2843 - val_accuracy: 0.8750\n",
            "Epoch 254/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.8802 - val_loss: 0.2843 - val_accuracy: 0.8750\n",
            "Epoch 255/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.8802 - val_loss: 0.2842 - val_accuracy: 0.8750\n",
            "Epoch 256/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.8802 - val_loss: 0.2841 - val_accuracy: 0.8750\n",
            "Epoch 257/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.8802 - val_loss: 0.2841 - val_accuracy: 0.8750\n",
            "Epoch 258/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.8802 - val_loss: 0.2841 - val_accuracy: 0.8750\n",
            "Epoch 259/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.8802 - val_loss: 0.2840 - val_accuracy: 0.8750\n",
            "Epoch 260/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.8802 - val_loss: 0.2839 - val_accuracy: 0.8750\n",
            "Epoch 261/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.8802 - val_loss: 0.2839 - val_accuracy: 0.8750\n",
            "Epoch 262/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.8802 - val_loss: 0.2839 - val_accuracy: 0.8750\n",
            "Epoch 263/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.8802 - val_loss: 0.2838 - val_accuracy: 0.8750\n",
            "Epoch 264/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.8802 - val_loss: 0.2838 - val_accuracy: 0.8750\n",
            "Epoch 265/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.8802 - val_loss: 0.2837 - val_accuracy: 0.8750\n",
            "Epoch 266/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.8802 - val_loss: 0.2837 - val_accuracy: 0.8750\n",
            "Epoch 267/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.8802 - val_loss: 0.2836 - val_accuracy: 0.8750\n",
            "Epoch 268/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8802 - val_loss: 0.2836 - val_accuracy: 0.8750\n",
            "Epoch 269/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.8802 - val_loss: 0.2835 - val_accuracy: 0.8750\n",
            "Epoch 270/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8802 - val_loss: 0.2835 - val_accuracy: 0.8750\n",
            "Epoch 271/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8802 - val_loss: 0.2835 - val_accuracy: 0.8750\n",
            "Epoch 272/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8802 - val_loss: 0.2834 - val_accuracy: 0.8750\n",
            "Epoch 273/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8802 - val_loss: 0.2834 - val_accuracy: 0.8750\n",
            "Epoch 274/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8802 - val_loss: 0.2833 - val_accuracy: 0.8750\n",
            "Epoch 275/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8802 - val_loss: 0.2833 - val_accuracy: 0.8750\n",
            "Epoch 276/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8802 - val_loss: 0.2832 - val_accuracy: 0.8750\n",
            "Epoch 277/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8802 - val_loss: 0.2832 - val_accuracy: 0.8750\n",
            "Epoch 278/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8802 - val_loss: 0.2832 - val_accuracy: 0.8750\n",
            "Epoch 279/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8802 - val_loss: 0.2831 - val_accuracy: 0.8750\n",
            "Epoch 280/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8802 - val_loss: 0.2831 - val_accuracy: 0.8750\n",
            "Epoch 281/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.8802 - val_loss: 0.2831 - val_accuracy: 0.8750\n",
            "Epoch 282/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.8802 - val_loss: 0.2830 - val_accuracy: 0.8750\n",
            "Epoch 283/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.8802 - val_loss: 0.2830 - val_accuracy: 0.8750\n",
            "Epoch 284/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.8802 - val_loss: 0.2829 - val_accuracy: 0.8750\n",
            "Epoch 285/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.8802 - val_loss: 0.2829 - val_accuracy: 0.8750\n",
            "Epoch 286/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.8802 - val_loss: 0.2829 - val_accuracy: 0.8750\n",
            "Epoch 287/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.8802 - val_loss: 0.2828 - val_accuracy: 0.8750\n",
            "Epoch 288/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2615 - accuracy: 0.8802 - val_loss: 0.2828 - val_accuracy: 0.8750\n",
            "Epoch 289/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8802 - val_loss: 0.2827 - val_accuracy: 0.8750\n",
            "Epoch 290/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8802 - val_loss: 0.2827 - val_accuracy: 0.8750\n",
            "Epoch 291/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8802 - val_loss: 0.2827 - val_accuracy: 0.8750\n",
            "Epoch 292/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8802 - val_loss: 0.2827 - val_accuracy: 0.8750\n",
            "Epoch 293/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2613 - accuracy: 0.8802 - val_loss: 0.2826 - val_accuracy: 0.8750\n",
            "Epoch 294/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2613 - accuracy: 0.8802 - val_loss: 0.2826 - val_accuracy: 0.8750\n",
            "Epoch 295/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2613 - accuracy: 0.8802 - val_loss: 0.2825 - val_accuracy: 0.8750\n",
            "Epoch 296/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8802 - val_loss: 0.2825 - val_accuracy: 0.8750\n",
            "Epoch 297/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2613 - accuracy: 0.8802 - val_loss: 0.2825 - val_accuracy: 0.8750\n",
            "Epoch 298/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8802 - val_loss: 0.2825 - val_accuracy: 0.8750\n",
            "Epoch 299/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.8802 - val_loss: 0.2824 - val_accuracy: 0.8750\n",
            "Epoch 300/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.8802 - val_loss: 0.2824 - val_accuracy: 0.8750\n",
            "Epoch 301/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.8802 - val_loss: 0.2824 - val_accuracy: 0.8750\n",
            "Epoch 302/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.8802 - val_loss: 0.2823 - val_accuracy: 0.8750\n",
            "Epoch 303/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8802 - val_loss: 0.2823 - val_accuracy: 0.8750\n",
            "Epoch 304/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8802 - val_loss: 0.2823 - val_accuracy: 0.8750\n",
            "Epoch 305/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8802 - val_loss: 0.2822 - val_accuracy: 0.8750\n",
            "Epoch 306/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8802 - val_loss: 0.2822 - val_accuracy: 0.8750\n",
            "Epoch 307/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8802 - val_loss: 0.2822 - val_accuracy: 0.8750\n",
            "Epoch 308/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.8802 - val_loss: 0.2822 - val_accuracy: 0.8750\n",
            "Epoch 309/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2822 - val_accuracy: 0.8750\n",
            "Epoch 310/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8802 - val_loss: 0.2821 - val_accuracy: 0.8750\n",
            "Epoch 311/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2821 - val_accuracy: 0.8750\n",
            "Epoch 312/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2821 - val_accuracy: 0.8750\n",
            "Epoch 313/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2820 - val_accuracy: 0.8750\n",
            "Epoch 314/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2820 - val_accuracy: 0.8750\n",
            "Epoch 315/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2820 - val_accuracy: 0.8750\n",
            "Epoch 316/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2820 - val_accuracy: 0.8750\n",
            "Epoch 317/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2820 - val_accuracy: 0.8750\n",
            "Epoch 318/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2819 - val_accuracy: 0.8750\n",
            "Epoch 319/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2819 - val_accuracy: 0.8750\n",
            "Epoch 320/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2819 - val_accuracy: 0.8750\n",
            "Epoch 321/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2818 - val_accuracy: 0.8750\n",
            "Epoch 322/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8802 - val_loss: 0.2818 - val_accuracy: 0.8750\n",
            "Epoch 323/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2818 - val_accuracy: 0.8750\n",
            "Epoch 324/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2818 - val_accuracy: 0.8750\n",
            "Epoch 325/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2818 - val_accuracy: 0.8750\n",
            "Epoch 326/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2817 - val_accuracy: 0.8750\n",
            "Epoch 327/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2817 - val_accuracy: 0.8750\n",
            "Epoch 328/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2817 - val_accuracy: 0.8750\n",
            "Epoch 329/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2817 - val_accuracy: 0.8750\n",
            "Epoch 330/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2817 - val_accuracy: 0.8750\n",
            "Epoch 331/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2816 - val_accuracy: 0.8750\n",
            "Epoch 332/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2612 - accuracy: 0.8802 - val_loss: 0.2816 - val_accuracy: 0.8750\n",
            "Epoch 333/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2816 - val_accuracy: 0.8750\n",
            "Epoch 334/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2816 - val_accuracy: 0.8750\n",
            "Epoch 335/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2815 - val_accuracy: 0.8750\n",
            "Epoch 336/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2815 - val_accuracy: 0.8750\n",
            "Epoch 337/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2815 - val_accuracy: 0.8542\n",
            "Epoch 338/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2815 - val_accuracy: 0.8542\n",
            "Epoch 339/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2815 - val_accuracy: 0.8542\n",
            "Epoch 340/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2814 - val_accuracy: 0.8542\n",
            "Epoch 341/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2814 - val_accuracy: 0.8542\n",
            "Epoch 342/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2814 - val_accuracy: 0.8542\n",
            "Epoch 343/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2814 - val_accuracy: 0.8542\n",
            "Epoch 344/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2814 - val_accuracy: 0.8542\n",
            "Epoch 345/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2813 - val_accuracy: 0.8542\n",
            "Epoch 346/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2813 - val_accuracy: 0.8542\n",
            "Epoch 347/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2813 - val_accuracy: 0.8542\n",
            "Epoch 348/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2813 - val_accuracy: 0.8542\n",
            "Epoch 349/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2813 - val_accuracy: 0.8542\n",
            "Epoch 350/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2813 - val_accuracy: 0.8542\n",
            "Epoch 351/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2812 - val_accuracy: 0.8542\n",
            "Epoch 352/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2812 - val_accuracy: 0.8542\n",
            "Epoch 353/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2812 - val_accuracy: 0.8542\n",
            "Epoch 354/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2812 - val_accuracy: 0.8542\n",
            "Epoch 355/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2812 - val_accuracy: 0.8542\n",
            "Epoch 356/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2812 - val_accuracy: 0.8542\n",
            "Epoch 357/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2812 - val_accuracy: 0.8542\n",
            "Epoch 358/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2811 - val_accuracy: 0.8542\n",
            "Epoch 359/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2811 - val_accuracy: 0.8542\n",
            "Epoch 360/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8802 - val_loss: 0.2811 - val_accuracy: 0.8542\n",
            "Epoch 361/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2811 - val_accuracy: 0.8542\n",
            "Epoch 362/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2811 - val_accuracy: 0.8542\n",
            "Epoch 363/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2810 - val_accuracy: 0.8542\n",
            "Epoch 364/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2810 - val_accuracy: 0.8542\n",
            "Epoch 365/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2810 - val_accuracy: 0.8542\n",
            "Epoch 366/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2810 - val_accuracy: 0.8542\n",
            "Epoch 367/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2810 - val_accuracy: 0.8542\n",
            "Epoch 368/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2810 - val_accuracy: 0.8542\n",
            "Epoch 369/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2810 - val_accuracy: 0.8542\n",
            "Epoch 370/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2810 - val_accuracy: 0.8542\n",
            "Epoch 371/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2809 - val_accuracy: 0.8542\n",
            "Epoch 372/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2809 - val_accuracy: 0.8542\n",
            "Epoch 373/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2809 - val_accuracy: 0.8542\n",
            "Epoch 374/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2809 - val_accuracy: 0.8542\n",
            "Epoch 375/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2809 - val_accuracy: 0.8542\n",
            "Epoch 376/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2809 - val_accuracy: 0.8542\n",
            "Epoch 377/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2808 - val_accuracy: 0.8542\n",
            "Epoch 378/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2808 - val_accuracy: 0.8542\n",
            "Epoch 379/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2808 - val_accuracy: 0.8542\n",
            "Epoch 380/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2808 - val_accuracy: 0.8542\n",
            "Epoch 381/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2808 - val_accuracy: 0.8542\n",
            "Epoch 382/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2808 - val_accuracy: 0.8542\n",
            "Epoch 383/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8802 - val_loss: 0.2808 - val_accuracy: 0.8542\n",
            "Epoch 384/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2808 - val_accuracy: 0.8542\n",
            "Epoch 385/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2808 - val_accuracy: 0.8542\n",
            "Epoch 386/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2807 - val_accuracy: 0.8542\n",
            "Epoch 387/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2807 - val_accuracy: 0.8542\n",
            "Epoch 388/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2807 - val_accuracy: 0.8542\n",
            "Epoch 389/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2807 - val_accuracy: 0.8542\n",
            "Epoch 390/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2807 - val_accuracy: 0.8542\n",
            "Epoch 391/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2807 - val_accuracy: 0.8542\n",
            "Epoch 392/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2807 - val_accuracy: 0.8542\n",
            "Epoch 393/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2807 - val_accuracy: 0.8542\n",
            "Epoch 394/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
            "Epoch 395/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
            "Epoch 396/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
            "Epoch 397/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
            "Epoch 398/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
            "Epoch 399/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
            "Epoch 400/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
            "Epoch 401/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
            "Epoch 402/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
            "Epoch 403/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
            "Epoch 404/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 405/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 406/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 407/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 408/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 409/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 410/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 411/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 412/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 413/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 414/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
            "Epoch 415/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 416/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 417/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 418/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 419/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 420/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 421/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 422/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 423/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 424/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 425/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
            "Epoch 426/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 427/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 428/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 429/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 430/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 431/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 432/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 433/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 434/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 435/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 436/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 437/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 438/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 439/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 440/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 441/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2803 - val_accuracy: 0.8542\n",
            "Epoch 442/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 443/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 444/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 445/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 446/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 447/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 448/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 449/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 450/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 451/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 452/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 453/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
            "Epoch 454/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 455/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 456/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 457/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 458/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 459/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 460/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 461/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 462/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 463/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 464/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 465/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 466/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 467/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 468/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 469/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
            "Epoch 470/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 471/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 472/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 473/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 474/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 475/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 476/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 477/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 478/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 479/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 480/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 481/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 482/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 483/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 484/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 485/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 486/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 487/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 488/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2800 - val_accuracy: 0.8542\n",
            "Epoch 489/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 490/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 491/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 492/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 493/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 494/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 495/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 496/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 497/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 498/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 499/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
            "Epoch 500/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8802 - val_loss: 0.2799 - val_accuracy: 0.8542\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f16b070f5e0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgmAIMmBU2D5"
      },
      "source": [
        "* **(k)** Calculer les prédictions faites par le modèle et les stocker dans **`test_pred`**.\n",
        "> Nous disposons de  y_test et de test_pred. Cependant avant de pouvoir évaluer notre modèle, il va falloir à partir des predcitions, qui sont des probabilité d'appartenance à une classe, retrouver la classe prédite.\n",
        "\n",
        "* **(l)** Grâce à la méthode **where** du module **numpy**. A **test_pred**, associer les classes et les prédictions faites pour chaque observation de l'échantillon test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-trBJ_NoU2D5"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "DZ1iCo-MU2D5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f2dad8-818e-485e-b7ce-fedf7bc9ce2a"
      },
      "source": [
        "test_pred = model.predict(X_test)\n",
        "classes_pred = np.where(test_pred>=0.5, 1,0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roiGDITlU2D5"
      },
      "source": [
        "* **(m)** Afficher le rapport de classification du modèle ainsi que la matrice de confusion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_246vTRxU2D6"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7xnAJHnBU2D6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab566692-fdc2-4688-94d1-ee2ca012d2d3"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test, classes_pred))\n",
        "print(confusion_matrix(y_test,classes_pred))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92        26\n",
            "           1       0.94      0.94      0.94        34\n",
            "\n",
            "    accuracy                           0.93        60\n",
            "   macro avg       0.93      0.93      0.93        60\n",
            "weighted avg       0.93      0.93      0.93        60\n",
            "\n",
            "[[24  2]\n",
            " [ 2 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFw6M8VTU2D6"
      },
      "source": [
        ">Le score pour ce modèle semble plutôt correct mais est-il plus ou moins performant qu'une régréssion logistique classique ?\n",
        ">\n",
        ">Pour vérifier cela, nous allons implémenter un modèle de régréssion logistique sur nos données.\n",
        ">\n",
        "* Exécuter la cellule suivante :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wCAsvVLvU2D6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "cf913a56-d95a-4472-ec66-4bbe94482931"
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.LogisticRegression(C = 1.0)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rl = clf.predict(X_test) # Prédictions à l'aide de la régression logistique\n",
        "\n",
        "cm = pd.crosstab(y_test, y_pred_rl, rownames=['Classe réelle'], colnames=['Classe prédite'])\n",
        "cm # Matrice de confusion\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classe prédite   0   1\n",
              "Classe réelle         \n",
              "0               24   2\n",
              "1                3  31"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-3a6fc0a3-c034-4114-8a9b-dca1144dc591\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Classe prédite</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Classe réelle</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a6fc0a3-c034-4114-8a9b-dca1144dc591')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-8aaf7c66-8300-493d-9539-b1166d4b4e0e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8aaf7c66-8300-493d-9539-b1166d4b4e0e')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-8aaf7c66-8300-493d-9539-b1166d4b4e0e button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3a6fc0a3-c034-4114-8a9b-dca1144dc591 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3a6fc0a3-c034-4114-8a9b-dca1144dc591');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "1K4Z0zWjU2D6"
      },
      "source": [
        ">Les résultats sont donc identiques, ce qui confirme bien les suppositions de départ.\n",
        ">Seulement, l'algorithme de Perceptron simple n'est plus utilisé en pratique.\n",
        ">\n",
        "> L'interêt de l'algorithme du Perceptron vient d'une technique démontrée en 1989 par George Cybenko qui consiste à empiler plusieurs perceptrons qui auront la même entrée sur une couche appelée **couche cachée** (*hidden layer* en anglais).\n",
        ">\n",
        "> La sortie de cette couche de perceptrons sera ensuite donnée en entrée à un perceptron qui fera la classification binaire. Ce perceptron forme ce que l'on appelle la **couche de sortie** (*output layer* en anglais).\n",
        ">\n",
        "> Un algorithme de ce type s'appelle **Perceptron Multicouche** (*Multilayer Perceptron* en anglais), souvent abrégé par l'acronyme **MLP**.\n",
        ">\n",
        "> **Vous verrez ces modèles dans les prochains modules !**"
      ]
    }
  ]
}